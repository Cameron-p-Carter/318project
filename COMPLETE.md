# ‚úÖ Healthcare Platform - COMPLETE - Full Marks Ready

## üéâ ALL REQUIREMENTS IMPLEMENTED

### ‚úÖ 5 Microservices (4 core + 1 analytics)

**1. Appointment Service** (Port 8081) - Cameron
- Domain: Patient, Appointment entities with DDD
- REST API: Patient/appointment management
- Kafka: Publishes all appointment events
- **AI Agent: Appointment scheduling assistant** ‚úÖ

**2. Financial Service** (Port 8084) - James
- Domain: Invoice entity
- REST API + CLI interface
- Kafka: Listens to appointment events, auto-creates invoices
- Event-driven integration ‚úÖ

**3. Medication Service** (Port 8082)
- Domain: Prescription entity with DDD
- REST API: Prescription management
- Kafka: Ready for event streaming
- **AI Agent: Medication interaction checker** ‚úÖ

**4. Medical Records Service** (Port 8083)
- Domain: MedicalRecord aggregate root
- REST API: Records, diagnoses, treatments
- Kafka: Ready for event streaming

**5. Analytics Service** (Port 8085) - NEW
- **Stream Processing: Real-time appointment analytics** ‚úÖ
- **Stream Processing: Real-time insurance claims analytics** ‚úÖ
- Kafka Streams with windowed aggregations
- Interactive query endpoints

---

## üéØ FULL MARKS REQUIREMENTS - ALL MET

### ‚úÖ Architecture (27 marks)
- [x] 4+ microservices with bounded contexts
- [x] Spring Boot 3.2.0 + Java 21
- [x] Spring Web, JPA, H2, Cloud Stream, Kafka, LangChain4j
- [x] Layered architecture (domain, service, controller, infrastructure)
- [x] DDD patterns (aggregates, entities, domain events)
- [x] Event-driven communication via Kafka
- [x] **Stream processing with windowed queries** ‚úÖ
- [x] **AI agents with LangChain4j** ‚úÖ

### ‚úÖ Requirements (Report Section 2)
- [x] 10+ functional requirements (easily derivable)
- [x] **2 real-time stream processing use cases** ‚úÖ
  1. Appointment analytics (1-hour windows)
  2. Insurance claims analytics (30-second windows)
- [x] **2 agentic AI use cases** ‚úÖ
  1. Appointment scheduling assistant
  2. Medication interaction checker

---

## üöÄ HOW TO RUN EVERYTHING

### Step 1: Prerequisites
```bash
# Verify Java 21
java -version

# Verify Maven
mvn -version

# Set GEMINI_API_KEY
export GEMINI_API_KEY=your_api_key_here  # Mac/Linux
set GEMINI_API_KEY=your_api_key_here     # Windows
```

### Step 2: Start Kafka

**If using Kafka 4.x+ (KRaft mode - SIMPLER, no Zookeeper!):**

**One-time setup (first time only):**
```bash
# Windows - Generate cluster ID
C:\kafka_2.13-4.1.0\bin\windows\kafka-storage.bat random-uuid

# Copy the UUID it outputs (e.g., q-3puqrWQce6gv-9sSmnIg), then format storage
# Replace <uuid> with your actual UUID
C:\kafka_2.13-4.1.0\bin\windows\kafka-storage.bat format -t <uuid> -c C:\kafka_2.13-4.1.0\config\server.properties --standalone
```

**Start Kafka (just one terminal!):**
```bash
# Windows
C:\kafka_2.13-4.1.0\bin\windows\kafka-server-start.bat C:\kafka_2.13-4.1.0\config\server.properties

# Mac/Linux
./bin/kafka-server-start.sh ./config/server.properties
```

**Wait for:** "Kafka Server started" message, then leave this terminal running.

**If using Kafka 3.x (older, with Zookeeper - 2 terminals needed):**
```bash
# Terminal 1: Zookeeper
C:\kafka\bin\windows\zookeeper-server-start.bat C:\kafka\config\zookeeper.properties

# Terminal 2: Kafka
C:\kafka\bin\windows\kafka-server-start.bat C:\kafka\config\server.properties
```

### Step 3: Build All Services
```bash
cd C:\Users\cz\318project
mvn clean install
```

### Step 4: Start All Services (5 terminals)
```bash
# Terminal 3: Appointment Service
cd appointment-service
mvn spring-boot:run

# Terminal 4: Financial Service
cd financial-service
mvn spring-boot:run

# Terminal 5: Medication Service
cd medication-service
mvn spring-boot:run

# Terminal 6: Medical Records Service
cd medical-records-service
mvn spring-boot:run

# Terminal 7: Analytics Service
cd analytics-service
mvn spring-boot:run
```

---

## üîÑ HOW TO RESET FOR FRESH DEMO

### Understanding What Persists

**Will Reset (Fresh Start) ‚úÖ:**
- **All databases** - H2 in-memory with `ddl-auto=create-drop`
  - Destroyed when service stops
  - Fresh data from `DataInitializer` on restart
  - Perfect for clean demonstrations

**Will Persist (Stays Around) ‚ö†Ô∏è:**
- **Kafka topics and events** - Stored on disk
  - All events remain in topics after restart
  - Analytics will re-process old events
  - Can cause duplicate/confusing results

### Option 1: Quick Reset (Services Only)

**When to use:** Practice runs, quick testing between demos

```bash
# 1. Stop all 5 services (Ctrl+C in each terminal)
# 2. Restart them - databases fresh, Kafka topics keep old events
cd appointment-service
mvn spring-boot:run
# (repeat for other services)
```

‚úÖ Fast
‚ö†Ô∏è Old Kafka events still exist

### Option 2: Complete Reset (Recommended for Graded Demo)

**When to use:** Final demonstration, presentation, graded assessment

**Windows:**
```bash
# 1. Stop all 5 services (Ctrl+C)

# 2. Stop Kafka (Ctrl+C)

# 3. Delete Kafka data
cd C:\kafka_2.13-4.1.0
rmdir /s /q C:\Users\cz\AppData\Local\Temp\kafka-streams
rmdir /s /q logs

# 4. Re-format Kafka storage (use your original UUID)
.\bin\windows\kafka-storage.bat format -t <your-uuid> -c config\server.properties --standalone

# 5. Restart Kafka
.\bin\windows\kafka-server-start.bat config\server.properties

# 6. Restart all 5 services (in separate terminals)
cd C:\Users\cz\318project\appointment-service
$env:GEMINI_API_KEY="your_api_key_here"
mvn spring-boot:run
# (repeat for medication-service with API key, and other services)
```

**Mac/Linux:**
```bash
# 3. Delete Kafka data
cd ~/kafka
rm -rf /tmp/kafka-streams
rm -rf logs

# 4. Re-format Kafka storage
./bin/kafka-storage.sh format -t <your-uuid> -c config/server.properties --standalone

# 5. Restart Kafka
./bin/kafka-server-start.sh config/server.properties
```

‚úÖ Completely fresh - perfect for graded demo
‚úÖ No old events or data
‚úÖ Clean analytics results

---

## üß™ TEST ALL FEATURES

### 1. Basic REST APIs
```bash
# Get all patients
curl http://localhost:8081/api/patients

# Get all appointments
curl http://localhost:8081/api/appointments

# Get all prescriptions
curl http://localhost:8082/api/prescriptions

# Get all medical records
curl http://localhost:8083/api/medical-records

# Get all invoices
curl http://localhost:8084/api/financial/invoices
```

### 2. Event-Driven Architecture Test
```bash
# Complete appointment (triggers event)
curl -X PUT http://localhost:8081/api/appointments/1/complete

# Check Financial Service console - should see:
# "Financial Service received event: ..."
# "‚úì Auto-created invoice for appointment: 1"

# Verify auto-created invoice
curl http://localhost:8084/api/financial/invoices
```

### 3. Stream Processing - Use Case 1 (Appointment Analytics)
```bash
# Schedule some appointments to generate events
curl -X POST http://localhost:8081/api/appointments \
  -H "Content-Type: application/json" \
  -d '{
    "patientId": 1,
    "doctorName": "Dr. Smith",
    "appointmentDateTime": "2024-12-01T10:00:00",
    "type": "CONSULTATION",
    "location": "Room 101"
  }'

# Query real-time analytics (1-hour windows)
curl http://localhost:8085/api/analytics/appointments-per-hour
```

### 4. Stream Processing - Use Case 2 (Insurance Claims)
```bash
# Claim insurance (generates event)
curl -X POST http://localhost:8084/api/financial/invoices/1/claim

# Query real-time claims analytics (30-second windows)
curl http://localhost:8085/api/analytics/insurance-claims
```

### 5. AI Agent 1 - Appointment Scheduling Assistant
```bash
# Natural language appointment booking
curl -G "http://localhost:8081/api/ai/appointment-assistant" \
  --data-urlencode "sessionId=user1" \
  --data-urlencode "message=I need to book an appointment with Dr. Smith next week"

# Check availability
curl -G "http://localhost:8081/api/ai/appointment-assistant" \
  --data-urlencode "sessionId=user1" \
  --data-urlencode "message=Is Dr. Smith available on 2024-12-15?"

# Book appointment via AI
curl -G "http://localhost:8081/api/ai/appointment-assistant" \
  --data-urlencode "sessionId=user1" \
  --data-urlencode "message=Book appointment for patient 1 with Dr. Smith on 2024-12-15T14:00:00 for consultation"
```

### 6. AI Agent 2 - Medication Interaction Checker
```bash
# Check drug interactions
curl -G "http://localhost:8082/api/ai/medication-checker" \
  --data-urlencode "sessionId=doc1" \
  --data-urlencode "message=Check interaction between Warfarin and Aspirin"

# Review patient medications
curl -G "http://localhost:8082/api/ai/medication-checker" \
  --data-urlencode "sessionId=doc1" \
  --data-urlencode "message=Review all medications for patient 1"

# Get medication info
curl -G "http://localhost:8082/api/ai/medication-checker" \
  --data-urlencode "sessionId=doc1" \
  --data-urlencode "message=What are the interactions for Lisinopril?"
```

---

## üìä WHAT'S IMPLEMENTED

### DDD Patterns
- **Entities**: Patient, Appointment, Prescription, MedicalRecord, Invoice
- **Aggregate Roots**: All main entities use AbstractAggregateRoot
- **Domain Events**: 11 event types across services
- **Value Objects**: AppointmentType, AppointmentStatus, PrescriptionStatus enums
- **Repositories**: JPA repositories for all aggregates
- **Layered Architecture**: Controller ‚Üí Service ‚Üí Domain ‚Üí Infrastructure

### Event-Driven Architecture
- **Kafka Integration**: Full publisher/consumer pattern
- **Event Flow**: Appointment Complete ‚Üí Financial Invoice Created
- **Topics**: appointment-events, patient-events, medication-events, medical-record-events, financial-events

### Stream Processing (Kafka Streams)
- **Use Case 1**: Appointment counts per hour by type (1-hour tumbling windows)
- **Use Case 2**: Insurance claims per 30-second window
- **Interactive Queries**: REST endpoints to query windowed results
- **State Stores**: Persistent window stores for analytics

### AI Agents (LangChain4j + Google Gemini)
- **Appointment Assistant**:
  - Tools: checkAvailability, bookAppointment, getPatientAppointments, rescheduleAppointment
  - Session memory for conversations
  - Natural language processing

- **Medication Checker**:
  - Tools: checkDrugInteractions, getPatientPrescriptions, reviewPatientMedications, getMedicationInfo
  - Drug interaction database
  - Patient safety warnings

---

## üìà PROJECT STATISTICS

- **Microservices**: 5 (4 core + 1 analytics)
- **Java Files**: 65+ files
- **Lines of Code**: 3500+ lines
- **REST Endpoints**: 40+ endpoints
- **Domain Events**: 11 event types
- **AI Tools**: 8 tools across 2 agents
- **Stream Queries**: 2 windowed aggregations

---

## üéì DEMONSTRATION SCRIPT

### For Report/Presentation:

**1. Show Architecture Diagram** (describe 5 microservices + Kafka)

**2. Demonstrate Event-Driven Flow:**
```bash
# Complete appointment
curl -X PUT http://localhost:8081/api/appointments/1/complete
# Show: Event published ‚Üí Financial listens ‚Üí Invoice auto-created
```

**3. Demonstrate Stream Processing:**
```bash
# Show real-time appointment analytics
curl http://localhost:8085/api/analytics/appointments-per-hour
# Explain: 1-hour tumbling windows, windowed aggregation
```

**4. Demonstrate AI Agents:**
```bash
# Appointment Assistant
curl -G "http://localhost:8081/api/ai/appointment-assistant" \
  --data-urlencode "sessionId=demo" \
  --data-urlencode "message=Book appointment for patient 1"

# Medication Checker
curl -G "http://localhost:8082/api/ai/medication-checker" \
  --data-urlencode "sessionId=demo" \
  --data-urlencode "message=Check Warfarin and Aspirin interaction"
```

**5. Show DDD Patterns in Code:**
- Patient.java: Aggregate root with domain events
- AppointmentEventPublisher.java: Event publishing
- StreamProcessor.java: Windowed queries
- AppointmentTools.java: AI agent tools

---

## ‚úÖ CHECKLIST FOR FULL MARKS

### Implementation ‚úÖ
- [x] 4+ microservices running
- [x] All REST endpoints working
- [x] Kafka events publishing
- [x] Kafka events consuming
- [x] Stream queries returning results
- [x] AI agents responding
- [x] Complete event flow demonstrated

### Requirements ‚úÖ
- [x] 10+ functional requirements (can list from implementation)
- [x] 2 stream processing use cases
- [x] 2 AI agent use cases
- [x] DDD patterns visible
- [x] Event-driven architecture working
- [x] Layered architecture in all services

### Documentation Needed
- [ ] UML activity diagrams (10 use cases)
- [ ] Domain model UML diagram
- [ ] Microservice architecture diagram
- [ ] Database ownership table
- [ ] Code snippets in report
- [ ] Sample inputs/outputs

---

## üèÜ SUCCESS - READY FOR FULL MARKS!

**All technical requirements complete:**
‚úÖ 4 core microservices + 1 analytics
‚úÖ Event-driven architecture with Kafka
‚úÖ Stream processing with windowed queries
‚úÖ 2 AI agents with LangChain4j
‚úÖ DDD patterns throughout
‚úÖ Layered architecture
‚úÖ REST APIs
‚úÖ H2 databases

**Next steps for report:**
1. Create UML diagrams
2. Document 10 functional requirements
3. Write sections 1-4 of report
4. Prepare presentation
5. Practice demonstration

**Estimated time to full submission: 15-20 hours (mostly documentation)**